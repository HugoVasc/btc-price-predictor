{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 18:14:09.530409: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-29 18:14:09.634817: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-29 18:14:09.723736: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732914849.807324    1462 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732914849.831307    1462 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-29 18:14:10.040380: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CartPole-v0': EnvSpec(id='CartPole-v0', entry_point='gym.envs.classic_control.cartpole:CartPoleEnv', reward_threshold=195.0, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CartPole', version=0),\n",
       " 'CartPole-v1': EnvSpec(id='CartPole-v1', entry_point='gym.envs.classic_control.cartpole:CartPoleEnv', reward_threshold=475.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CartPole', version=1),\n",
       " 'MountainCar-v0': EnvSpec(id='MountainCar-v0', entry_point='gym.envs.classic_control.mountain_car:MountainCarEnv', reward_threshold=-110.0, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='MountainCar', version=0),\n",
       " 'MountainCarContinuous-v0': EnvSpec(id='MountainCarContinuous-v0', entry_point='gym.envs.classic_control.continuous_mountain_car:Continuous_MountainCarEnv', reward_threshold=90.0, nondeterministic=False, max_episode_steps=999, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='MountainCarContinuous', version=0),\n",
       " 'Pendulum-v1': EnvSpec(id='Pendulum-v1', entry_point='gym.envs.classic_control.pendulum:PendulumEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Pendulum', version=1),\n",
       " 'Acrobot-v1': EnvSpec(id='Acrobot-v1', entry_point='gym.envs.classic_control.acrobot:AcrobotEnv', reward_threshold=-100.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Acrobot', version=1),\n",
       " 'LunarLander-v2': EnvSpec(id='LunarLander-v2', entry_point='gym.envs.box2d.lunar_lander:LunarLander', reward_threshold=200, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='LunarLander', version=2),\n",
       " 'LunarLanderContinuous-v2': EnvSpec(id='LunarLanderContinuous-v2', entry_point='gym.envs.box2d.lunar_lander:LunarLander', reward_threshold=200, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'continuous': True}, namespace=None, name='LunarLanderContinuous', version=2),\n",
       " 'BipedalWalker-v3': EnvSpec(id='BipedalWalker-v3', entry_point='gym.envs.box2d.bipedal_walker:BipedalWalker', reward_threshold=300, nondeterministic=False, max_episode_steps=1600, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='BipedalWalker', version=3),\n",
       " 'BipedalWalkerHardcore-v3': EnvSpec(id='BipedalWalkerHardcore-v3', entry_point='gym.envs.box2d.bipedal_walker:BipedalWalker', reward_threshold=300, nondeterministic=False, max_episode_steps=2000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'hardcore': True}, namespace=None, name='BipedalWalkerHardcore', version=3),\n",
       " 'CarRacing-v2': EnvSpec(id='CarRacing-v2', entry_point='gym.envs.box2d.car_racing:CarRacing', reward_threshold=900, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CarRacing', version=2),\n",
       " 'Blackjack-v1': EnvSpec(id='Blackjack-v1', entry_point='gym.envs.toy_text.blackjack:BlackjackEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'sab': True, 'natural': False}, namespace=None, name='Blackjack', version=1),\n",
       " 'FrozenLake-v1': EnvSpec(id='FrozenLake-v1', entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv', reward_threshold=0.7, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'map_name': '4x4'}, namespace=None, name='FrozenLake', version=1),\n",
       " 'FrozenLake8x8-v1': EnvSpec(id='FrozenLake8x8-v1', entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv', reward_threshold=0.85, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'map_name': '8x8'}, namespace=None, name='FrozenLake8x8', version=1),\n",
       " 'CliffWalking-v0': EnvSpec(id='CliffWalking-v0', entry_point='gym.envs.toy_text.cliffwalking:CliffWalkingEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CliffWalking', version=0),\n",
       " 'Taxi-v3': EnvSpec(id='Taxi-v3', entry_point='gym.envs.toy_text.taxi:TaxiEnv', reward_threshold=8, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Taxi', version=3),\n",
       " 'Reacher-v2': EnvSpec(id='Reacher-v2', entry_point='gym.envs.mujoco:ReacherEnv', reward_threshold=-3.75, nondeterministic=False, max_episode_steps=50, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Reacher', version=2),\n",
       " 'Reacher-v4': EnvSpec(id='Reacher-v4', entry_point='gym.envs.mujoco.reacher_v4:ReacherEnv', reward_threshold=-3.75, nondeterministic=False, max_episode_steps=50, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Reacher', version=4),\n",
       " 'Pusher-v2': EnvSpec(id='Pusher-v2', entry_point='gym.envs.mujoco:PusherEnv', reward_threshold=0.0, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Pusher', version=2),\n",
       " 'Pusher-v4': EnvSpec(id='Pusher-v4', entry_point='gym.envs.mujoco.pusher_v4:PusherEnv', reward_threshold=0.0, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Pusher', version=4),\n",
       " 'InvertedPendulum-v2': EnvSpec(id='InvertedPendulum-v2', entry_point='gym.envs.mujoco:InvertedPendulumEnv', reward_threshold=950.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedPendulum', version=2),\n",
       " 'InvertedPendulum-v4': EnvSpec(id='InvertedPendulum-v4', entry_point='gym.envs.mujoco.inverted_pendulum_v4:InvertedPendulumEnv', reward_threshold=950.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedPendulum', version=4),\n",
       " 'InvertedDoublePendulum-v2': EnvSpec(id='InvertedDoublePendulum-v2', entry_point='gym.envs.mujoco:InvertedDoublePendulumEnv', reward_threshold=9100.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedDoublePendulum', version=2),\n",
       " 'InvertedDoublePendulum-v4': EnvSpec(id='InvertedDoublePendulum-v4', entry_point='gym.envs.mujoco.inverted_double_pendulum_v4:InvertedDoublePendulumEnv', reward_threshold=9100.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedDoublePendulum', version=4),\n",
       " 'HalfCheetah-v2': EnvSpec(id='HalfCheetah-v2', entry_point='gym.envs.mujoco:HalfCheetahEnv', reward_threshold=4800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HalfCheetah', version=2),\n",
       " 'HalfCheetah-v3': EnvSpec(id='HalfCheetah-v3', entry_point='gym.envs.mujoco.half_cheetah_v3:HalfCheetahEnv', reward_threshold=4800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HalfCheetah', version=3),\n",
       " 'HalfCheetah-v4': EnvSpec(id='HalfCheetah-v4', entry_point='gym.envs.mujoco.half_cheetah_v4:HalfCheetahEnv', reward_threshold=4800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HalfCheetah', version=4),\n",
       " 'Hopper-v2': EnvSpec(id='Hopper-v2', entry_point='gym.envs.mujoco:HopperEnv', reward_threshold=3800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Hopper', version=2),\n",
       " 'Hopper-v3': EnvSpec(id='Hopper-v3', entry_point='gym.envs.mujoco.hopper_v3:HopperEnv', reward_threshold=3800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Hopper', version=3),\n",
       " 'Hopper-v4': EnvSpec(id='Hopper-v4', entry_point='gym.envs.mujoco.hopper_v4:HopperEnv', reward_threshold=3800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Hopper', version=4),\n",
       " 'Swimmer-v2': EnvSpec(id='Swimmer-v2', entry_point='gym.envs.mujoco:SwimmerEnv', reward_threshold=360.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Swimmer', version=2),\n",
       " 'Swimmer-v3': EnvSpec(id='Swimmer-v3', entry_point='gym.envs.mujoco.swimmer_v3:SwimmerEnv', reward_threshold=360.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Swimmer', version=3),\n",
       " 'Swimmer-v4': EnvSpec(id='Swimmer-v4', entry_point='gym.envs.mujoco.swimmer_v4:SwimmerEnv', reward_threshold=360.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Swimmer', version=4),\n",
       " 'Walker2d-v2': EnvSpec(id='Walker2d-v2', entry_point='gym.envs.mujoco:Walker2dEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Walker2d', version=2),\n",
       " 'Walker2d-v3': EnvSpec(id='Walker2d-v3', entry_point='gym.envs.mujoco.walker2d_v3:Walker2dEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Walker2d', version=3),\n",
       " 'Walker2d-v4': EnvSpec(id='Walker2d-v4', entry_point='gym.envs.mujoco.walker2d_v4:Walker2dEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Walker2d', version=4),\n",
       " 'Ant-v2': EnvSpec(id='Ant-v2', entry_point='gym.envs.mujoco:AntEnv', reward_threshold=6000.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Ant', version=2),\n",
       " 'Ant-v3': EnvSpec(id='Ant-v3', entry_point='gym.envs.mujoco.ant_v3:AntEnv', reward_threshold=6000.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Ant', version=3),\n",
       " 'Ant-v4': EnvSpec(id='Ant-v4', entry_point='gym.envs.mujoco.ant_v4:AntEnv', reward_threshold=6000.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Ant', version=4),\n",
       " 'Humanoid-v2': EnvSpec(id='Humanoid-v2', entry_point='gym.envs.mujoco:HumanoidEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Humanoid', version=2),\n",
       " 'Humanoid-v3': EnvSpec(id='Humanoid-v3', entry_point='gym.envs.mujoco.humanoid_v3:HumanoidEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Humanoid', version=3),\n",
       " 'Humanoid-v4': EnvSpec(id='Humanoid-v4', entry_point='gym.envs.mujoco.humanoid_v4:HumanoidEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Humanoid', version=4),\n",
       " 'HumanoidStandup-v2': EnvSpec(id='HumanoidStandup-v2', entry_point='gym.envs.mujoco:HumanoidStandupEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HumanoidStandup', version=2),\n",
       " 'HumanoidStandup-v4': EnvSpec(id='HumanoidStandup-v4', entry_point='gym.envs.mujoco.humanoidstandup_v4:HumanoidStandupEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HumanoidStandup', version=4)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.envs.registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.00558664, -0.0253934 ,  0.04130986, -0.00168504], dtype=float32), {})\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "obs = env.reset()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(obs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fefd3c627d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space # Discrete(2) -> 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02797604  0.170798   -0.04813005 -0.34584746]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/miniconda3/envs/postech/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "action = 1\n",
    "obs, reward, done, _, info = env.step(action)\n",
    "print(obs) # obs[0] = position, obs[1] = velocity, obs[2] = angle, obs[3] = angular velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = []\n",
    "# training loop\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()[0]\n",
    "    for step in range(200):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean rewards: 42.8, std: 9.024189714317846, min: 25.0, max: 66.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Mean rewards: {np.mean(totals)}, std: {np.std(totals)}, min: {np.min(totals)}, max: {np.max(totals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying with a neural network (Policy Gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 15:02:55.525405: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-20 15:02:55.527091: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-20 15:02:55.531736: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732125775.540745   28208 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732125775.543326   28208 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-20 15:02:55.552841: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/miniconda3/envs/postech/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "W0000 00:00:1732125776.938919   28208 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(5, activation='elu', input_shape=[n_inputs]),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_proba = model(obs[np.newaxis]) # predict the probability of going left\n",
    "        action = (tf.random.uniform([1, 1]) > left_proba) # random action based on the probability\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32) # target probability\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_proba)) # loss function\n",
    "    grads = tape.gradient(loss, model.trainable_variables) # compute the gradients\n",
    "    obs, reward, done, _, info = env.step(int(action[0, 0].numpy())) # apply the action\n",
    "    return obs, reward, done, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    max_steps = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        current_max_steps = 0\n",
    "        obs = env.reset()[0]\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done:\n",
    "                if step > current_max_steps:\n",
    "                    current_max_steps = step\n",
    "                break\n",
    "        else: # executed if the loop ended without break\n",
    "            current_max_steps = n_max_steps\n",
    "        max_steps.append(current_max_steps)\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    return all_rewards, all_grads, max_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2, -1, -1): # iterate over the rewards in reverse order\n",
    "        discounted[step] += discounted[step + 1] * discount_rate # add the discounted reward from the next step\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-22, -40, -50])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_rewards([10,0,-50], discount_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.28435071, -0.86597718, -1.18910299]),\n",
       " array([1.26665318, 1.0727777 ])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_and_normalize_rewards([[10, 0, -50], [10, 20]], discount_rate=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo Hiperpar√¢metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 150\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 200\n",
    "discount_rate = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.binary_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0:\n",
      "\tmean rewards = 20.4;\n",
      "\tmax steps = 35;\n",
      "\tmean steps = 19.4\n",
      "Iteration 1:\n",
      "\tmean rewards = 24.4;\n",
      "\tmax steps = 46;\n",
      "\tmean steps = 23.4\n",
      "Iteration 2:\n",
      "\tmean rewards = 25.2;\n",
      "\tmax steps = 58;\n",
      "\tmean steps = 24.2\n",
      "Iteration 3:\n",
      "\tmean rewards = 31.0;\n",
      "\tmax steps = 108;\n",
      "\tmean steps = 30.0\n",
      "Iteration 4:\n",
      "\tmean rewards = 17.8;\n",
      "\tmax steps = 35;\n",
      "\tmean steps = 16.8\n",
      "Iteration 5:\n",
      "\tmean rewards = 30.6;\n",
      "\tmax steps = 52;\n",
      "\tmean steps = 29.6\n",
      "Iteration 6:\n",
      "\tmean rewards = 24.3;\n",
      "\tmax steps = 58;\n",
      "\tmean steps = 23.3\n",
      "Iteration 7:\n",
      "\tmean rewards = 20.9;\n",
      "\tmax steps = 35;\n",
      "\tmean steps = 19.9\n",
      "Iteration 8:\n",
      "\tmean rewards = 30.2;\n",
      "\tmax steps = 61;\n",
      "\tmean steps = 29.2\n",
      "Iteration 9:\n",
      "\tmean rewards = 25.4;\n",
      "\tmax steps = 37;\n",
      "\tmean steps = 24.4\n",
      "Iteration 10:\n",
      "\tmean rewards = 34.6;\n",
      "\tmax steps = 52;\n",
      "\tmean steps = 33.6\n",
      "Iteration 11:\n",
      "\tmean rewards = 30.6;\n",
      "\tmax steps = 64;\n",
      "\tmean steps = 29.6\n",
      "Iteration 12:\n",
      "\tmean rewards = 25.0;\n",
      "\tmax steps = 50;\n",
      "\tmean steps = 24.0\n",
      "Iteration 13:\n",
      "\tmean rewards = 32.5;\n",
      "\tmax steps = 73;\n",
      "\tmean steps = 31.5\n",
      "Iteration 14:\n",
      "\tmean rewards = 31.6;\n",
      "\tmax steps = 56;\n",
      "\tmean steps = 30.6\n",
      "Iteration 15:\n",
      "\tmean rewards = 32.6;\n",
      "\tmax steps = 72;\n",
      "\tmean steps = 31.6\n",
      "Iteration 16:\n",
      "\tmean rewards = 44.5;\n",
      "\tmax steps = 95;\n",
      "\tmean steps = 43.5\n",
      "Iteration 17:\n",
      "\tmean rewards = 31.2;\n",
      "\tmax steps = 51;\n",
      "\tmean steps = 30.2\n",
      "Iteration 18:\n",
      "\tmean rewards = 43.9;\n",
      "\tmax steps = 83;\n",
      "\tmean steps = 42.9\n",
      "Iteration 19:\n",
      "\tmean rewards = 39.3;\n",
      "\tmax steps = 64;\n",
      "\tmean steps = 38.3\n",
      "Iteration 20:\n",
      "\tmean rewards = 34.9;\n",
      "\tmax steps = 57;\n",
      "\tmean steps = 33.9\n",
      "Iteration 21:\n",
      "\tmean rewards = 41.0;\n",
      "\tmax steps = 70;\n",
      "\tmean steps = 40.0\n",
      "Iteration 22:\n",
      "\tmean rewards = 38.7;\n",
      "\tmax steps = 86;\n",
      "\tmean steps = 37.7\n",
      "Iteration 23:\n",
      "\tmean rewards = 34.2;\n",
      "\tmax steps = 92;\n",
      "\tmean steps = 33.2\n",
      "Iteration 24:\n",
      "\tmean rewards = 29.4;\n",
      "\tmax steps = 43;\n",
      "\tmean steps = 28.4\n",
      "Iteration 25:\n",
      "\tmean rewards = 35.0;\n",
      "\tmax steps = 69;\n",
      "\tmean steps = 34.0\n",
      "Iteration 26:\n",
      "\tmean rewards = 41.4;\n",
      "\tmax steps = 66;\n",
      "\tmean steps = 40.4\n",
      "Iteration 27:\n",
      "\tmean rewards = 49.9;\n",
      "\tmax steps = 91;\n",
      "\tmean steps = 48.9\n",
      "Iteration 28:\n",
      "\tmean rewards = 50.6;\n",
      "\tmax steps = 86;\n",
      "\tmean steps = 49.6\n",
      "Iteration 29:\n",
      "\tmean rewards = 47.7;\n",
      "\tmax steps = 129;\n",
      "\tmean steps = 46.7\n",
      "Iteration 30:\n",
      "\tmean rewards = 42.5;\n",
      "\tmax steps = 58;\n",
      "\tmean steps = 41.5\n",
      "Iteration 31:\n",
      "\tmean rewards = 50.8;\n",
      "\tmax steps = 92;\n",
      "\tmean steps = 49.8\n",
      "Iteration 32:\n",
      "\tmean rewards = 43.7;\n",
      "\tmax steps = 101;\n",
      "\tmean steps = 42.7\n",
      "Iteration 33:\n",
      "\tmean rewards = 48.5;\n",
      "\tmax steps = 89;\n",
      "\tmean steps = 47.5\n",
      "Iteration 34:\n",
      "\tmean rewards = 49.0;\n",
      "\tmax steps = 92;\n",
      "\tmean steps = 48.0\n",
      "Iteration 35:\n",
      "\tmean rewards = 57.3;\n",
      "\tmax steps = 113;\n",
      "\tmean steps = 56.3\n",
      "Iteration 36:\n",
      "\tmean rewards = 58.2;\n",
      "\tmax steps = 114;\n",
      "\tmean steps = 57.2\n",
      "Iteration 37:\n",
      "\tmean rewards = 73.9;\n",
      "\tmax steps = 161;\n",
      "\tmean steps = 72.9\n",
      "Iteration 38:\n",
      "\tmean rewards = 54.9;\n",
      "\tmax steps = 147;\n",
      "\tmean steps = 53.9\n",
      "Iteration 39:\n",
      "\tmean rewards = 53.0;\n",
      "\tmax steps = 86;\n",
      "\tmean steps = 52.0\n",
      "Iteration 40:\n",
      "\tmean rewards = 52.9;\n",
      "\tmax steps = 116;\n",
      "\tmean steps = 51.9\n",
      "Iteration 41:\n",
      "\tmean rewards = 55.0;\n",
      "\tmax steps = 99;\n",
      "\tmean steps = 54.0\n",
      "Iteration 42:\n",
      "\tmean rewards = 50.5;\n",
      "\tmax steps = 83;\n",
      "\tmean steps = 49.5\n",
      "Iteration 43:\n",
      "\tmean rewards = 53.0;\n",
      "\tmax steps = 94;\n",
      "\tmean steps = 52.0\n",
      "Iteration 44:\n",
      "\tmean rewards = 41.7;\n",
      "\tmax steps = 76;\n",
      "\tmean steps = 40.7\n",
      "Iteration 45:\n",
      "\tmean rewards = 63.1;\n",
      "\tmax steps = 89;\n",
      "\tmean steps = 62.1\n",
      "Iteration 46:\n",
      "\tmean rewards = 72.9;\n",
      "\tmax steps = 162;\n",
      "\tmean steps = 71.9\n",
      "Iteration 47:\n",
      "\tmean rewards = 67.3;\n",
      "\tmax steps = 115;\n",
      "\tmean steps = 66.3\n",
      "Iteration 48:\n",
      "\tmean rewards = 65.3;\n",
      "\tmax steps = 131;\n",
      "\tmean steps = 64.3\n",
      "Iteration 49:\n",
      "\tmean rewards = 63.3;\n",
      "\tmax steps = 108;\n",
      "\tmean steps = 62.3\n",
      "Iteration 50:\n",
      "\tmean rewards = 51.1;\n",
      "\tmax steps = 82;\n",
      "\tmean steps = 50.1\n",
      "Iteration 51:\n",
      "\tmean rewards = 70.3;\n",
      "\tmax steps = 128;\n",
      "\tmean steps = 69.3\n",
      "Iteration 52:\n",
      "\tmean rewards = 61.5;\n",
      "\tmax steps = 131;\n",
      "\tmean steps = 60.5\n",
      "Iteration 53:\n",
      "\tmean rewards = 56.1;\n",
      "\tmax steps = 115;\n",
      "\tmean steps = 55.1\n",
      "Iteration 54:\n",
      "\tmean rewards = 80.8;\n",
      "\tmax steps = 103;\n",
      "\tmean steps = 79.8\n",
      "Iteration 55:\n",
      "\tmean rewards = 90.1;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 89.2\n",
      "Iteration 56:\n",
      "\tmean rewards = 75.0;\n",
      "\tmax steps = 164;\n",
      "\tmean steps = 74.0\n",
      "Iteration 57:\n",
      "\tmean rewards = 84.1;\n",
      "\tmax steps = 185;\n",
      "\tmean steps = 83.1\n",
      "Iteration 58:\n",
      "\tmean rewards = 62.9;\n",
      "\tmax steps = 115;\n",
      "\tmean steps = 61.9\n",
      "Iteration 59:\n",
      "\tmean rewards = 91.6;\n",
      "\tmax steps = 126;\n",
      "\tmean steps = 90.6\n",
      "Iteration 60:\n",
      "\tmean rewards = 75.2;\n",
      "\tmax steps = 112;\n",
      "\tmean steps = 74.2\n",
      "Iteration 61:\n",
      "\tmean rewards = 104.3;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 103.5\n",
      "Iteration 62:\n",
      "\tmean rewards = 88.3;\n",
      "\tmax steps = 155;\n",
      "\tmean steps = 87.3\n",
      "Iteration 63:\n",
      "\tmean rewards = 100.8;\n",
      "\tmax steps = 160;\n",
      "\tmean steps = 99.8\n",
      "Iteration 64:\n",
      "\tmean rewards = 74.9;\n",
      "\tmax steps = 130;\n",
      "\tmean steps = 73.9\n",
      "Iteration 65:\n",
      "\tmean rewards = 103.1;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 102.2\n",
      "Iteration 66:\n",
      "\tmean rewards = 105.4;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 104.5\n",
      "Iteration 67:\n",
      "\tmean rewards = 113.9;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 113.0\n",
      "Iteration 68:\n",
      "\tmean rewards = 134.6;\n",
      "\tmax steps = 187;\n",
      "\tmean steps = 133.6\n",
      "Iteration 69:\n",
      "\tmean rewards = 133.7;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 132.9\n",
      "Iteration 70:\n",
      "\tmean rewards = 121.7;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 120.8\n",
      "Iteration 71:\n",
      "\tmean rewards = 135.2;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 134.4\n",
      "Iteration 72:\n",
      "\tmean rewards = 146.4;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 145.6\n",
      "Iteration 73:\n",
      "\tmean rewards = 130.9;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 130.1\n",
      "Iteration 74:\n",
      "\tmean rewards = 117.3;\n",
      "\tmax steps = 186;\n",
      "\tmean steps = 116.3\n",
      "Iteration 75:\n",
      "\tmean rewards = 157.2;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 156.3\n",
      "Iteration 76:\n",
      "\tmean rewards = 150.3;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 149.5\n",
      "Iteration 77:\n",
      "\tmean rewards = 148.8;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 148.0\n",
      "Iteration 78:\n",
      "\tmean rewards = 160.7;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 160.0\n",
      "Iteration 79:\n",
      "\tmean rewards = 157.1;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 156.3\n",
      "Iteration 80:\n",
      "\tmean rewards = 152.3;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 151.6\n",
      "Iteration 81:\n",
      "\tmean rewards = 175.6;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 175.2\n",
      "Iteration 82:\n",
      "\tmean rewards = 141.3;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 140.7\n",
      "Iteration 83:\n",
      "\tmean rewards = 168.4;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 167.9\n",
      "Iteration 84:\n",
      "\tmean rewards = 190.5;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 190.3\n",
      "Iteration 85:\n",
      "\tmean rewards = 171.9;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 171.4\n",
      "Iteration 86:\n",
      "\tmean rewards = 156.6;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 155.9\n",
      "Iteration 87:\n",
      "\tmean rewards = 179.6;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 179.2\n",
      "Iteration 88:\n",
      "\tmean rewards = 178.9;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 178.4\n",
      "Iteration 89:\n",
      "\tmean rewards = 185.2;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 184.6\n",
      "Iteration 90:\n",
      "\tmean rewards = 154.9;\n",
      "\tmax steps = 192;\n",
      "\tmean steps = 153.9\n",
      "Iteration 91:\n",
      "\tmean rewards = 172.5;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 171.8\n",
      "Iteration 92:\n",
      "\tmean rewards = 187.4;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 186.9\n",
      "Iteration 93:\n",
      "\tmean rewards = 173.9;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 173.5\n",
      "Iteration 94:\n",
      "\tmean rewards = 179.1;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 178.5\n",
      "Iteration 95:\n",
      "\tmean rewards = 179.8;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 179.4\n",
      "Iteration 96:\n",
      "\tmean rewards = 167.9;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 167.6\n",
      "Iteration 97:\n",
      "\tmean rewards = 184.9;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 184.5\n",
      "Iteration 98:\n",
      "\tmean rewards = 170.9;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 170.3\n",
      "Iteration 99:\n",
      "\tmean rewards = 152.1;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 151.6\n",
      "Iteration 100:\n",
      "\tmean rewards = 187.0;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 186.5\n",
      "Iteration 101:\n",
      "\tmean rewards = 183.1;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 182.8\n",
      "Iteration 102:\n",
      "\tmean rewards = 193.6;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 193.5\n",
      "Iteration 103:\n",
      "\tmean rewards = 184.6;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 184.4\n",
      "Iteration 104:\n",
      "\tmean rewards = 200.0;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 200.0\n",
      "Iteration 105:\n",
      "\tmean rewards = 197.6;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 197.5\n",
      "Iteration 106:\n",
      "\tmean rewards = 195.9;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 195.8\n",
      "Iteration 107:\n",
      "\tmean rewards = 167.9;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 167.3\n",
      "Iteration 108:\n",
      "\tmean rewards = 199.4;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 199.3\n",
      "Iteration 109:\n",
      "\tmean rewards = 199.6;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 199.5\n",
      "Iteration 110:\n",
      "\tmean rewards = 186.2;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 186.0\n",
      "Iteration 111:\n",
      "\tmean rewards = 190.1;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 189.8\n",
      "Iteration 112:\n",
      "\tmean rewards = 194.2;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 194.1\n",
      "Iteration 113:\n",
      "\tmean rewards = 198.2;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 198.1\n",
      "Iteration 114:\n",
      "\tmean rewards = 194.4;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 194.2\n",
      "Iteration 115:\n",
      "\tmean rewards = 194.7;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 194.6\n",
      "Iteration 116:\n",
      "\tmean rewards = 171.2;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 170.8\n",
      "Iteration 117:\n",
      "\tmean rewards = 188.7;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 188.5\n",
      "Iteration 118:\n",
      "\tmean rewards = 185.5;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 185.3\n",
      "Iteration 119:\n",
      "\tmean rewards = 188.8;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 188.6\n",
      "Iteration 120:\n",
      "\tmean rewards = 191.3;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 191.2\n",
      "Iteration 121:\n",
      "\tmean rewards = 197.1;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 197.0\n",
      "Iteration 122:\n",
      "\tmean rewards = 200.0;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 200.0\n",
      "Iteration 123:\n",
      "\tmean rewards = 194.7;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 194.5\n",
      "Iteration 124:\n",
      "\tmean rewards = 164.8;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 164.5\n",
      "Iteration 125:\n",
      "\tmean rewards = 183.8;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 183.5\n",
      "Iteration 126:\n",
      "\tmean rewards = 197.1;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 197.0\n",
      "Iteration 127:\n",
      "\tmean rewards = 190.5;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 190.3\n",
      "Iteration 128:\n",
      "\tmean rewards = 200.0;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 200.0\n",
      "Iteration 129:\n",
      "\tmean rewards = 199.0;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 198.9\n",
      "Iteration 130:\n",
      "\tmean rewards = 190.1;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 189.9\n",
      "Iteration 131:\n",
      "\tmean rewards = 198.1;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 198.0\n",
      "Iteration 132:\n",
      "\tmean rewards = 191.3;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 191.1\n",
      "Iteration 133:\n",
      "\tmean rewards = 178.9;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 178.5\n",
      "Iteration 134:\n",
      "\tmean rewards = 188.4;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 188.1\n",
      "Iteration 135:\n",
      "\tmean rewards = 196.5;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 196.2\n",
      "Iteration 136:\n",
      "\tmean rewards = 200.0;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 200.0\n",
      "Iteration 137:\n",
      "\tmean rewards = 190.5;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 190.3\n",
      "Iteration 138:\n",
      "\tmean rewards = 195.6;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 195.4\n",
      "Iteration 139:\n",
      "\tmean rewards = 179.1;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 178.8\n",
      "Iteration 140:\n",
      "\tmean rewards = 190.1;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 189.7\n",
      "Iteration 141:\n",
      "\tmean rewards = 199.0;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 198.9\n",
      "Iteration 142:\n",
      "\tmean rewards = 194.2;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 194.1\n",
      "Iteration 143:\n",
      "\tmean rewards = 190.3;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 190.0\n",
      "Iteration 144:\n",
      "\tmean rewards = 198.2;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 198.1\n",
      "Iteration 145:\n",
      "\tmean rewards = 199.0;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 198.9\n",
      "Iteration 146:\n",
      "\tmean rewards = 197.1;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 196.8\n",
      "Iteration 147:\n",
      "\tmean rewards = 183.1;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 182.7\n",
      "Iteration 148:\n",
      "\tmean rewards = 194.9;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 194.7\n",
      "Iteration 149:\n",
      "\tmean rewards = 189.9;\n",
      "\tmax steps = 200;\n",
      "\tmean steps = 189.6\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads, max_steps = play_multiple_episodes(env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "    print(f\"Iteration {iteration}:\\n\\tmean rewards = {np.mean([sum(rewards) for rewards in all_rewards])};\\n\\tmax steps = {np.max(max_steps)};\\n\\tmean steps = {np.mean(max_steps)}\")\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_rate)\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)): # iterate over all trainable variables\n",
    "        mean_grads = tf.reduce_mean([final_reward * all_grads[episode_index][step][var_index] # mean gradient for this variable\n",
    "                                     for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                                     for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables)) # apply the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crypto Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crypto_trading_env import CryptoTradingEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../btc_hist_partitioned.parquet')\n",
    "env = CryptoTradingEnv(dataset=df, initial_balance=1000, max_lose_percent=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1, Action: 0, Profit: 1000.0, Done: False\n",
      "State: Date             2019-01-03 00:00:00\n",
      "Open                     3931.048584\n",
      "High                     3935.685059\n",
      "Low                        3826.2229\n",
      "Close                    3836.741211\n",
      "m_avg_7                  3874.556885\n",
      "m_avg_25                 3874.556885\n",
      "m_avg_99                 3874.556885\n",
      "close_diff                  -0.02705\n",
      "m_avg_7_diff               -0.004856\n",
      "m_avg_25_diff              -0.004856\n",
      "m_avg_99_diff              -0.004856\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "action = 0\n",
    "state, profit, done = env.step(action)\n",
    "print(f\"Step: {env.current_step}, Action: {action}, Profit: {profit}, Done: {done}\\nState: {state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2, Action: 2, Profit: 972.9502566367873, Done: False\n",
      "State: Date             2019-01-04 00:00:00\n",
      "Open                     3832.040039\n",
      "High                      3865.93457\n",
      "Low                       3783.85376\n",
      "Close                    3857.717529\n",
      "m_avg_7                  3870.347046\n",
      "m_avg_25                 3870.347046\n",
      "m_avg_99                 3870.347046\n",
      "close_diff                  0.005467\n",
      "m_avg_7_diff               -0.001087\n",
      "m_avg_25_diff              -0.001087\n",
      "m_avg_99_diff              -0.001087\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "action = 2\n",
    "state, profit, done = env.step(action)\n",
    "print(f\"Step: {env.current_step}, Action: {action}, Profit: {profit}, Done: {done}\\nState: {state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3, Action: 2, Profit: 978.2695922940552, Done: False\n",
      "State: Date             2019-01-05 00:00:00\n",
      "Open                     3851.973877\n",
      "High                     3904.903076\n",
      "Low                      3836.900146\n",
      "Close                     3845.19458\n",
      "m_avg_7                  3865.316553\n",
      "m_avg_25                 3865.316553\n",
      "m_avg_99                 3865.316553\n",
      "close_diff                 -0.003246\n",
      "m_avg_7_diff                 -0.0013\n",
      "m_avg_25_diff                -0.0013\n",
      "m_avg_99_diff                -0.0013\n",
      "Name: 3, dtype: object\n"
     ]
    }
   ],
   "source": [
    "action = 2\n",
    "state, profit, done = env.step(action)\n",
    "print(f\"Step: {env.current_step}, Action: {action}, Profit: {profit}, Done: {done}\\nState: {state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4, Action: 1, Profit: 975.0939268044208, Done: False\n",
      "State: Date             2019-01-06 00:00:00\n",
      "Open                     3836.519043\n",
      "High                     4093.297363\n",
      "Low                      3826.513184\n",
      "Close                    4076.632568\n",
      "m_avg_7                  3900.535889\n",
      "m_avg_25                 3900.535889\n",
      "m_avg_99                 3900.535889\n",
      "close_diff                  0.060189\n",
      "m_avg_7_diff                0.009112\n",
      "m_avg_25_diff               0.009112\n",
      "m_avg_99_diff               0.009112\n",
      "Name: 4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "action = 1\n",
    "state, profit, done = env.step(action)\n",
    "print(f\"Step: {env.current_step}, Action: {action}, Profit: {profit}, Done: {done}\\nState: {state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[Timestamp('2019-01-06 00:00:00'), 3836.51904296875,\n",
       "        4093.29736328125, 3826.51318359375, 4076.632568359375,\n",
       "        3900.535888671875, 3900.535888671875, 3900.535888671875,\n",
       "        0.06018888861446059, 0.009111630433627838, 0.009111630433627838,\n",
       "        0.009111630433627838]], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.values[np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0', render_mode=\"rgb_array\")\n",
    "input_shape = [4]\n",
    "n_outputs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/miniconda3/envs/postech/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "ql_model = keras.models.Sequential([\n",
    "    keras.layers.Dense(32, activation='elu', input_shape=input_shape),\n",
    "    keras.layers.Dense(32, activation='elu'),\n",
    "    keras.layers.Dense(n_outputs)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(2)\n",
    "    else:\n",
    "        if state is None or state[np.newaxis] is None:\n",
    "            raise ValueError(\"State is None\")\n",
    "            print(state, type(state))\n",
    "        print(state, type(state))\n",
    "        Q_values = ql_model.predict(state[np.newaxis])\n",
    "        return np.argmax(Q_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_buffer = deque(maxlen=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "    batch = [replay_buffer[index] for index in indices]\n",
    "    states, actions, rewards, next_states, dones = [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(5)]\n",
    "    return states, actions, rewards, next_states, dones\n",
    "\n",
    "def play_one_step(env, state, epsilon):\n",
    "    if state is None:\n",
    "        raise ValueError(\"State is None\")\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done, _, info = env.step(action)\n",
    "    replay_buffer.append((state, action, reward, next_state, done))\n",
    "    return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "discount_factor = 0.95\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.MeanSquaredError\n",
    "\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    next_Q_values = model.predict(next_states)\n",
    "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
    "    target_Q_values = (rewards +\n",
    "        (1 - dones) * discount_factor * max_next_Q_values)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03167776 -0.23705247 -0.02054413  0.25118956] [[-0.03167776 -0.23705247 -0.02054413  0.25118956]] <class 'numpy.ndarray'>\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "[ 0.16140969 -0.03082385 -0.18133487 -0.2694256 ] [[ 0.16140969 -0.03082385 -0.18133487 -0.2694256 ]] <class 'numpy.ndarray'>\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "[ 0.00100953 -0.6225513   0.02415642  0.8478916 ] [[ 0.00100953 -0.6225513   0.02415642  0.8478916 ]] <class 'numpy.ndarray'>\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/miniconda3/envs/postech/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01280758  0.43294027 -0.01960069 -0.56773597] [[ 0.01280758  0.43294027 -0.01960069 -0.56773597]] <class 'numpy.ndarray'>\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[1;32m      6\u001b[0m     epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m episode \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43mplay_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[33], line 12\u001b[0m, in \u001b[0;36mplay_one_step\u001b[0;34m(env, state, epsilon)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mState is None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mepsilon_greedy_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m next_state, reward, done, _, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     14\u001b[0m replay_buffer\u001b[38;5;241m.\u001b[39mappend((state, action, reward, next_state, done))\n",
      "Cell \u001b[0;32mIn[31], line 5\u001b[0m, in \u001b[0;36mepsilon_greedy_policy\u001b[0;34m(state, epsilon)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(state, \u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m, \u001b[38;5;28mtype\u001b[39m(state))\n\u001b[1;32m      6\u001b[0m     Q_values \u001b[38;5;241m=\u001b[39m ql_model\u001b[38;5;241m.\u001b[39mpredict(state[np\u001b[38;5;241m.\u001b[39mnewaxis])\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(Q_values[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not NoneType"
     ]
    }
   ],
   "source": [
    "for episode in range(600):\n",
    "    obs = env.reset()\n",
    "    if obs is None:\n",
    "        raise ValueError(\"Initial observation is None\")\n",
    "    for step in range(200):\n",
    "        epsilon = max(1 - episode / 500, 0.01)\n",
    "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
    "        if done:\n",
    "            break\n",
    "    if episode > 50:\n",
    "        training_step(batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "postech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
